{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rachelhakes/Machine-Learning/blob/main/Data_Prep.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import dependencies\n",
        "\n"
      ],
      "metadata": {
        "id": "u0TXg6ZeX6zi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6M4srjIfXy_e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "793933f5-dbc8-4fdc-8e40-4223c72d50a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.7/dist-packages (3.0.9)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl) (1.1.0)\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install openpyxl"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mount drive"
      ],
      "metadata": {
        "id": "VfZD0EKmYFA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "7f-Unl7FYGFg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acca8439-388b-40d6-ba74-00eb03412b47"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading the data set"
      ],
      "metadata": {
        "id": "hl2-Q1_TYR6G"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# The path is not the same for every student. Please change path when working with the dataset\n",
        "data_url = \"/content/drive/MyDrive/Classroom/Competition1_raw_data (1).xlsx\"\n",
        "\n",
        "competition_df = pd.read_excel(data_url)\n",
        "competition_df.head(10)"
      ],
      "metadata": {
        "id": "61BKLIR-YRZE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make a copy of the dataframe as we are about to drop NA in our target_df\n",
        "competition_df1 = competition_df.copy()"
      ],
      "metadata": {
        "id": "AMePZQOgkSkN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# We will replace the '-' with nan for our target variables\n",
        "competition_df['P(IPO)'] = competition_df['P(IPO)'].replace('-', np.NaN)\n",
        "competition_df['P(H)'] = competition_df['P(H)'].replace('-', np.NaN)\n",
        "competition_df['P(L)'] = competition_df['P(L)'].replace('-', np.NaN)\n",
        "competition_df['P(1Day)'] = competition_df['P(1Day)'].replace('-', np.NaN)"
      ],
      "metadata": {
        "id": "3xU-SY2jHj7a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can drop the nan rows before splitting the dataframe\n",
        "competition_df = competition_df.dropna(axis=0, subset=['P(IPO)', 'P(H)','P(L)', 'P(1Day)'])"
      ],
      "metadata": {
        "id": "8H3k6kUBIwaZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the dataframe into 3 different dataframe"
      ],
      "metadata": {
        "id": "oN49TdTsYg7x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the dataframe containing our target variables\n",
        "target_df = competition_df[['P(IPO)',\t'P(H)',\t'P(L)',\t'P(1Day)']]\n",
        "target_df.head()"
      ],
      "metadata": {
        "id": "xnUSGqZvZVbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now lets create categorical dataframe\n",
        "categorical_df = competition_df[['I1', 'I2', 'I3', 'C2']]\n",
        "categorical_df.head()"
      ],
      "metadata": {
        "id": "xDmeVRCeaZvx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create numerical dataframe\n",
        "numerical_df = competition_df[['C1', 'C3', 'C4', 'C5', 'C6', 'C7', 'T1', 'T2', 'T3', 'T4', 'T5', 'S1', 'S2', 'S3']]\n",
        "numerical_df.head()"
      ],
      "metadata": {
        "id": "F9vxukFTbp45"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handle '-' values for the **numerical_df** dataframe"
      ],
      "metadata": {
        "id": "kxEDpUVwYlgK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will replace the '-' with nan for our numerical dataframe\n",
        "numerical_df = numerical_df.replace('-', np.NaN)\n",
        "numerical_df.head(10)"
      ],
      "metadata": {
        "id": "OIePS2XPP5mS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the percent of NaN values per column\n",
        "(numerical_df.isna().sum()/numerical_df.shape[0]).round(4) * 100"
      ],
      "metadata": {
        "id": "YJkCJdoJ6QXi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Impute the continuous data with the median value\n",
        "numerical_df['C1'].fillna((numerical_df['C1'].median()), inplace=True)\n",
        "numerical_df['C3'].fillna((numerical_df['C3'].median()), inplace=True)\n",
        "numerical_df['C4'].fillna((numerical_df['C4'].median()), inplace=True)\n",
        "numerical_df['C5'].fillna((numerical_df['C5'].median()), inplace=True)\n",
        "numerical_df['C6'].fillna((numerical_df['C6'].median()), inplace=True)\n",
        "numerical_df['C7'].fillna((numerical_df['C7'].median()), inplace=True)\n",
        "numerical_df['T1'].fillna((numerical_df['T1'].median()), inplace=True)\n",
        "numerical_df['T2'].fillna((numerical_df['T2'].median()), inplace=True)\n",
        "numerical_df['T3'].fillna((numerical_df['T3'].median()), inplace=True)\n",
        "numerical_df['T4'].fillna((numerical_df['T4'].median()), inplace=True)\n",
        "numerical_df['T5'].fillna((numerical_df['T5'].median()), inplace=True)\n",
        "numerical_df['S1'].fillna((numerical_df['S1'].median()), inplace=True)\n",
        "numerical_df['S2'].fillna((numerical_df['S2'].median()), inplace=True)\n",
        "numerical_df['S3'].fillna((numerical_df['S3'].median()), inplace=True)"
      ],
      "metadata": {
        "id": "geJtG57ZYbTQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure no NaN data remains after imputation\n",
        "(numerical_df.isna().sum()/numerical_df.shape[0]).round(4) * 100"
      ],
      "metadata": {
        "id": "P0PasNTyYhkr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Handle '-' values for the **categorical_df** dataframe"
      ],
      "metadata": {
        "id": "ZRvZtXbhTrwK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# We will replace the '-' with nan for our categorical dataframe\n",
        "categorical_df = categorical_df.replace('-', np.NaN)\n",
        "categorical_df.head(10)"
      ],
      "metadata": {
        "id": "3_5lreR-650y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check the percent of NaN values per column\n",
        "(categorical_df.isna().sum()/categorical_df.shape[0]).round(4) * 100"
      ],
      "metadata": {
        "id": "yUcrhyS-T42b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## NOT NECESSARY SINCE THERE IS NO MISSING DATA\n",
        "\n",
        "\n",
        "# Since the dataframe represents categorical data, the mode will be used to impute the missing values\n",
        "categorical_df['C2'].fillna((categorical_df['C2'].mode()[0]), inplace=True)"
      ],
      "metadata": {
        "id": "tT-W6IU_UCMg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Ratio of missing values\n",
        "(categorical_df.isna().sum()/categorical_df.shape[0]).round(4) * 100"
      ],
      "metadata": {
        "id": "56-ik32yUPKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Check types of each dataframe"
      ],
      "metadata": {
        "id": "-xUUC9527e9L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target_df.dtypes"
      ],
      "metadata": {
        "id": "CfU71Xau7jAa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "numerical_df.dtypes"
      ],
      "metadata": {
        "id": "jmZc77aB7nXO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets convert all types into float64\n",
        "numerical_df = numerical_df.astype('float64')"
      ],
      "metadata": {
        "id": "U9smPhpYKW98"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check to see if types were properly converted to float\n",
        "numerical_df.dtypes"
      ],
      "metadata": {
        "id": "1Ha1YO1gKrSz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_df.dtypes"
      ],
      "metadata": {
        "id": "ekH09GPI7uiX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Since C2 is binary, we will have to convert that into str\n",
        "categorical_df['C2'] = categorical_df['C2'].astype(str)"
      ],
      "metadata": {
        "id": "cg-pCSSl740J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check to see if types were properly converted to str\n",
        "categorical_df.dtypes"
      ],
      "metadata": {
        "id": "E7-zqkwN8LnZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating new columns **target_df** and handling values"
      ],
      "metadata": {
        "id": "jFEc7d7wBGAR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# swap values of 'P(L)' and 'P(H)' if 'P(L)' is greater than 'P(H)'\n",
        "target_df['P(H)'], target_df['P(L)'] = np.where(target_df['P(H)'] < target_df['P(L)'], [target_df['P(L)'], target_df['P(H)']], [target_df['P(H)'], target_df['P(L)']])"
      ],
      "metadata": {
        "id": "39mTqaEb-9wY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check if the swapping works as expected\n",
        "assert target_df['P(H)'].any() >= target_df['P(L)'].any()"
      ],
      "metadata": {
        "id": "P8DK8-XbABDh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating column 'P(mid)' as we need it for creating other columns\n",
        "target_df['P(mid)'] = ((target_df['P(H)'] + target_df['P(L)']) / 2)\n",
        "target_df['Y1'] = np.where((target_df['P(IPO)'] < target_df['P(mid)']), 1, 0)\n",
        "target_df['Y2'] = np.where(target_df['P(IPO)'] < target_df['P(1Day)'], 1, 0)\n",
        "target_df.head()"
      ],
      "metadata": {
        "id": "usTlda0aBNMv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# target variable Y1 and Y2 are categorical\n",
        "target_df['Y1'] = target_df['Y1'].astype(str)\n",
        "target_df['Y2'] = target_df['Y2'].astype(str)"
      ],
      "metadata": {
        "id": "kgNaYRoICIcw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check to see if types were properly converted\n",
        "target_df.dtypes"
      ],
      "metadata": {
        "id": "69EzUx5MCn2D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure swapping works\n",
        "assert target_df['P(H)'].any() >= target_df['P(L)'].any()"
      ],
      "metadata": {
        "id": "jKm8D29B_8_J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# put the target variables in their own dataframe\n",
        "target_variables = target_df[['Y1', 'Y2']]\n",
        "target_variables.head()"
      ],
      "metadata": {
        "id": "dMdLPwDsrShc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# export dataframe for use in pipeline\n",
        "to_csv = target_variables.to_csv('/content/drive/MyDrive/Classroom/DATA 6545: Machine Learning for Predictive Analysis SP2022/targetvariables.csv', index = True, encoding='utf-8')"
      ],
      "metadata": {
        "id": "LCoB8GLLJ1At"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating new columns **numerical_df** and handling values"
      ],
      "metadata": {
        "id": "EFvVMC1FAmI_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# apply constraints, replace with NaN values if not met\n",
        "numerical_df['T1'] = np.where(numerical_df['T1'] <= 0, np.NaN, numerical_df['T1'])\n",
        "numerical_df['T2'] = np.where(numerical_df['T2'] <= 0, np.NaN, numerical_df['T2'])\n",
        "numerical_df['T3'] = np.where(numerical_df['T3'] < 0, np.NaN, numerical_df['T3'])\n",
        "numerical_df['T4'] = np.where(numerical_df['T4'] < 0, np.NaN, numerical_df['T4'])\n",
        "numerical_df['T5'] = np.where(numerical_df['T5'] < 0, np.NaN, numerical_df['T5'])\n",
        "numerical_df['S1'] = np.where(numerical_df['S1'] < 0, np.NaN, numerical_df['S1'])\n",
        "numerical_df['S2'] = np.where(numerical_df['S2'] < 0, np.NaN, numerical_df['S2'])\n",
        "numerical_df['S3'] = np.where(numerical_df['S3'] < 0, np.NaN, numerical_df['S3'])\n",
        "numerical_df['C5'] = np.where(numerical_df['C5'] <= 0, np.NaN, numerical_df['C5'])\n",
        "numerical_df['C6'] = np.where(numerical_df['C6'] <= 0, np.NaN, numerical_df['C6'])\n"
      ],
      "metadata": {
        "id": "nlwan8jT91KR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check minimum values in each column\n",
        "numerical_df.min()"
      ],
      "metadata": {
        "id": "4M35IAH_-3OW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Double check these variable do not have a value less than/less than or equal to 0\n",
        "assert numerical_df['T1'].any() >= 0\n",
        "assert numerical_df['T2'].any() >= 0\n",
        "assert numerical_df['T3'].any() > 0\n",
        "assert numerical_df['T4'].any() > 0\n",
        "assert numerical_df['T5'].any() > 0\n",
        "assert numerical_df['S1'].any() > 0\n",
        "assert numerical_df['S2'].any() > 0\n",
        "assert numerical_df['S3'].any() > 0\n",
        "assert numerical_df['C5'].any() >= 0\n",
        "assert numerical_df['C6'].any() >= 0"
      ],
      "metadata": {
        "id": "j-rMxpiS809K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# with so many attributes lets check for null values\n",
        "(numerical_df.isna().sum()/numerical_df.shape[0]).round(4) * 100"
      ],
      "metadata": {
        "id": "m1_-1Ui4cmrW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lets impute the rest with their median\n",
        "numerical_df['T2'].fillna((numerical_df['T2'].median()), inplace=True)\n",
        "numerical_df['T5'].fillna((numerical_df['T5'].median()), inplace=True)\n",
        "numerical_df['S1'].fillna((numerical_df['S1'].median()), inplace=True)"
      ],
      "metadata": {
        "id": "SSOLBjSldDRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test to see if imputation worked\n",
        "(numerical_df.isna().sum()/numerical_df.shape[0]).round(4) * 100"
      ],
      "metadata": {
        "id": "REod26-79efV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Now we can create the columns from the dictionary\n",
        "numerical_df['C1\\''] = np.log10(numerical_df['C1'])\n",
        "# Positive EPS Dummy Variable\n",
        "numerical_df['C3\\''] = np.where(numerical_df['C3'] > 0, 1, 0)\n",
        "# Set this variable to be binary/categorical\n",
        "numerical_df['C3\\''] = numerical_df['C3\\''].astype(str)\n",
        "# Share Overhand Variable\n",
        "numerical_df['C5\\''] = numerical_df['C5'] / numerical_df['C6']\n",
        "# Up Revision Variable\n",
        "numerical_df['C6\\''] = np.where(target_df['P(IPO)'] > target_df['P(mid)'], ((target_df['P(IPO)']-target_df['P(mid)'])/target_df['P(mid)'])*100, 0)\n",
        "# percent of long sentences\n",
        "numerical_df['T1\\''] = numerical_df['T4'] / numerical_df['T1']\n",
        "# percent of real words\n",
        "numerical_df['T2\\''] = numerical_df['T3'] / numerical_df['T2']\n",
        "# percent of long words\n",
        "numerical_df['T3\\''] = numerical_df['T5'] / numerical_df['T2']\n",
        "# percent of positive words\n",
        "numerical_df['S1\\''] = numerical_df['S1'] / numerical_df['T2']\n",
        "# percent of negative words\n",
        "numerical_df['S2\\''] = numerical_df['S2'] / numerical_df['T2']\n",
        "# percent of uncertain words\n",
        "numerical_df['S3\\''] = numerical_df['S3'] / numerical_df['T2']"
      ],
      "metadata": {
        "id": "F9j2PRJu_h2a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# make sure we didn't end up with NaN values during feature creation\n",
        "numerical_df.isna().sum()"
      ],
      "metadata": {
        "id": "bYB2cjgrwMsT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#### additional feature creation\n",
        "# ratio of positive to negative words\n",
        "numerical_df['S4\\''] = numerical_df['S1']/numerical_df['S2']\n",
        "# ratio of positive to uncertain words\n",
        "numerical_df['S5\\''] = numerical_df['S1']/numerical_df['S3']\n",
        "# ratio of negative to uncertain words\n",
        "numerical_df['S6\\''] = numerical_df['S2']/numerical_df['S3']"
      ],
      "metadata": {
        "id": "M7TtE2MLp1ES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Creation with I3 Binning"
      ],
      "metadata": {
        "id": "SxYo3t9NLnPa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# look at categorical dataframe\n",
        "categorical_df.head()"
      ],
      "metadata": {
        "id": "Mc7mjXmGDT3H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check dtypes again\n",
        "categorical_df.dtypes"
      ],
      "metadata": {
        "id": "Jz3VMVEkPUGs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create variables for each number\n",
        "replace_map = {2: \"Manufacturing\", 3: \"Manufacturing\", 7: \"Service\", 8: \"Service\"}\n",
        "replace_map2 = {35: \"Tech\", 36: \"Tech\", 37: \"Tech\", 38: \"Tech\", 73: \"Tech\"}"
      ],
      "metadata": {
        "id": "wICphIj_doMO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# replace values in rows with multiple I3 codes with the most fitting I3 code\n",
        "categorical_df.at[174, 'I3'] = 3861\n",
        "categorical_df.at[187, 'I3'] = 3651\n",
        "categorical_df.at[221, 'I3'] = 5400\n",
        "categorical_df.at[246, 'I3'] = 5400\n",
        "categorical_df.at[499, 'I3'] = 3663\n",
        "categorical_df.at[620, 'I3'] = 7389"
      ],
      "metadata": {
        "id": "biy0AHkp8NDm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# copy the 'I3' row from the categorical dataframe into 2 pandas series\n",
        "i4 = categorical_df['I3']\n",
        "i5 = categorical_df['I3']"
      ],
      "metadata": {
        "id": "Ux695wpEKXYd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an empty list\n",
        "result = []\n",
        "for i in i4:\n",
        "  # use dictionary to replace values with manufacturing vs. service\n",
        "  if int(i / 1000) in replace_map.keys():\n",
        "     result.append(replace_map[int(i / 1000)])\n",
        "  # if no value matches, replace with other\n",
        "  else:\n",
        "    result.append(\"Others\")\n",
        "print(len(result))"
      ],
      "metadata": {
        "id": "Q7C_-SUwixWD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create an empty list\n",
        "result2 = []\n",
        "for i in i5:\n",
        "  # use dictionary to replace values with tech\n",
        "  if int(i / 100) in replace_map2.keys():\n",
        "     result2.append(replace_map2[int(i / 100)])\n",
        "  # if no value matches, replace with non-tech\n",
        "  else:\n",
        "    result2.append(\"Non-techs\")\n",
        "print(len(result2))"
      ],
      "metadata": {
        "id": "swq-vkbvKd79"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# assign feature creations to columns in categorical dataframe\n",
        "categorical_df['I4'] = result\n",
        "categorical_df['I5'] = result2"
      ],
      "metadata": {
        "id": "ZwkL0QNPKjwa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_df.head(10)"
      ],
      "metadata": {
        "id": "SAEBEa3yixDX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One hot encoding"
      ],
      "metadata": {
        "id": "EFumfoKIELZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_df_onehot = categorical_df.copy()\n",
        "categorical_df_onehot = pd.get_dummies(categorical_df_onehot, columns=['I4', 'I5'], prefix=['I4', 'I5'])\n",
        "categorical_df_onehot.head()"
      ],
      "metadata": {
        "id": "NmwnjMroKp7W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merging all data together into one dataframe"
      ],
      "metadata": {
        "id": "mQZeOFwfLcRi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a new version of the dataset, we can now create a dataframe containing the new features"
      ],
      "metadata": {
        "id": "gqazKk-Nfiki"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "competition_df2 = pd.concat([target_df, categorical_df_onehot, numerical_df], axis=1)\n",
        "competition_df2.reset_index(drop=True, inplace=True)\n",
        "competition_df2"
      ],
      "metadata": {
        "id": "OD-3UQnvWHte"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Dataframes for Feature Selection and Evaluation"
      ],
      "metadata": {
        "id": "d758L7ETKxEa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## final numerical features\n",
        "features_df = competition_df2[['C1\\'', 'C4', 'C5\\'', 'C6\\'', 'C7', 'T1\\'', 'T2\\'', 'T3\\'', 'S1\\'', 'S2\\'', 'S3\\'', 'S4\\'', 'S5\\'', 'S6\\'']]\n",
        "features_df.head()"
      ],
      "metadata": {
        "id": "-hc2l0wxftSt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# export to csv for pipeline\n",
        "to_csv = features_df.to_csv('/content/drive/MyDrive/Classroom/DATA 6545: Machine Learning for Predictive Analysis SP2022/pipeline.csv', index = True, encoding='utf-8')\n"
      ],
      "metadata": {
        "id": "uJbx2hDpE3Az"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## final categorical features\n",
        "cat_features_df = competition_df2[['C2','C3\\'', 'I4_Manufacturing', 'I4_Others', 'I4_Service', 'I5_Non-techs', 'I5_Tech']]\n",
        "cat_features_df.head()"
      ],
      "metadata": {
        "id": "U_9a96QlIKHx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# export to csv for pipeline\n",
        "to_csv = cat_features_df.to_csv('/content/drive/MyDrive/Classroom/DATA 6545: Machine Learning for Predictive Analysis SP2022/categorical_df.csv', index = True, encoding='utf-8')"
      ],
      "metadata": {
        "id": "BtH49t1tPics"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}